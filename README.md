<h1 align='center'>OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling</h1>
<div align='center'>
    <a href='https://github.com/yangzhou24' target='_blank'>Yang Zhou</a><sup>1</sup>â€ƒ
    <a href='https://github.com/yyfz' target='_blank'>Yifan Wang</a><sup>1</sup>â€ƒ
    <a href='https://zhoutimemachine.github.io' target='_blank'>Jianjun Zhou</a><sup>1,2</sup>â€ƒ
    <a href='https://github.com/AmberHeart' target='_blank'>Wenzheng Chang</a><sup>1</sup>â€ƒ
    <a href='https://github.com/ghy0324' target='_blank'>Haoyu Guo</a><sup>1</sup>â€ƒ
    <a href='https://github.com/LiZizun' target='_blank'>Zizun Li</a><sup>1</sup>â€ƒ
    <a href='https://kaijing.space/' target='_blank'>Kaijing Ma</a><sup>1</sup>â€ƒ
    
</div>
<div align='center'>
<a href='https://scholar.google.com/citations?user=VuTRUg8AAAAJ' target='_blank'>Xinyue Li</a><sup>1</sup>â€ƒ
    <a href='https://scholar.google.com/citations?user=5SuBWh0AAAAJ' target='_blank'>Yating Wang</a><sup>1</sup>â€ƒ
    <a href='https://www.haoyizhu.site/' target='_blank'>Haoyi Zhu</a><sup>1</sup>â€ƒ
    <a href='https://mingyulau.github.io/' target='_blank'>Mingyu Liu</a><sup>1,2</sup>â€ƒ
    <a href='https://scholar.google.com/citations?user=FbSpETgAAAAJ' target='_blank'>Dingning Liu</a><sup>1</sup>â€ƒ
    <a href='https://yangjiangeyjg.github.io/' target='_blank'>Jiange Yang</a><sup>1</sup>
    <a href='https://github.com/Kr1sJFU' target='_blank'>Zhoujie Fu</a><sup>1</sup>â€ƒâ€ƒ
    
</div>
<div align='center'>
    <a href='https://sotamak1r.github.io/' target='_blank'>Junyi Chen</a><sup>1</sup>â€ƒ
    <a href='https://cshen.github.io' target='_blank'>Chunhua Shen</a><sup>2</sup>â€ƒ
    <a href='https://oceanpang.github.io' target='_blank'>Jiangmiao Pang</a><sup>1</sup>â€ƒ
    <a href='https://kpzhang93.github.io/' target='_blank'>Kaipeng Zhang</a><sup>1</sup>
    <a href='https://tonghe90.github.io/' target='_blank'>Tong He</a><sup>1â€ </sup>
</div>
<div align='center'>
    <sup>1</sup>Shanghai AI Labâ€ƒ <sup>2</sup>ZJUâ€ƒ
</div>
<br>
<div align="center">
  <a href="https://yangzhou24.github.io/OmniWorld/"><img src="https://img.shields.io/badge/Project Page-5745BB?logo=google-chrome&logoColor=white"></a> â€‚
  <a href="https://arxiv.org/abs/2509.12201"><img src="https://img.shields.io/static/v1?label=Paper&message=Arxiv&color=red&logo=arxiv"></a> â€‚
  <a href="https://github.com/yangzhou24/OmniWorld"><img src="https://img.shields.io/static/v1?label=Code&message=Github&color=blue&logo=github"></a> â€‚
  <a href="https://huggingface.co/datasets/InternRobotics/OmniWorld"><img src="https://img.shields.io/static/v1?label=Dataset&message=HuggingFace&color=yellow&logo=huggingface"></a> â€‚
</div>

<img src="assets/teaser.png" width="1000px">

## ğŸ‰ NEWS
- [2025.9.21] ğŸ”¥ The **OmniWorld-Game** dataset now includes **5k splits** in total on Hugging Face!
- [2025.9.16] ğŸ”¥ The first 1.2k splits release of **OmniWorld-Game** is now live on Hugging Face! **More data is coming soon, stay tuned!**

## âœ¨ Overview

OmniWorld is a large-scale, multi-domain, and multi-modal dataset specifically designed for ğŸŒ**4D world modeling**, e.g. 4D geometric reconstruction, future prediction & camera-controlled video generation.

### ğŸ”‘ Key Features

- ğŸ“Š **Massive Scale**: 4000+ hours, 600K+ sequences, 300M+ frames
- ğŸ¤– **Diverse Domains**: sourced from simulartor, robot, human & the Internet
- ğŸ¨ **Rich Multi-Modality**: depth maps, camera poses, text captions, optical flow & foreground mask

### ğŸ® Introducing _OmniWorld-Game_

_OmniWorld-Game_ is a newly collected high-quality synthetic subset of the main _OmniWorld_ dataset. It features:

- ğŸ“Š **Scale**: 214 hours, 96K video clips, 18M+ frames
- ğŸ§© **Resolution & Diversity**: 720P RGB image capatured from a wide range of dynamic game environments
- ğŸ¨ **Comprehensive Annotations**: cover all annotation types of the _OmniWorld_ dataset

### ğŸ† _OmniWorld-Game_ Benchmark

_OmniWorld-Game_ Benchmark offers 4D world modeling evaluation for 3D Geometric Prediction &
Camera Control Video Generation. Found: 

- ğŸš« Current state-of-the-art approaches **still show great limitations** in modeling complex 4D environments, based on both quantitative metrics and qualitative results.
- ğŸ“ˆ **Fine-tuning** existing SOTA methods on _OmniWorld_ leads to **significant performance gains** across 4D reconstruction and video generation tasks, highlighting the value of our dataset.


## ğŸ’¡ Dataset Download
You can download the entire OmniWorld dataset using the following command:
```bash
# 1. Install (if you haven't yet)
pip install --upgrade "huggingface_hub[cli]"

# 2. Full download
hf download InternRobotics/OmniWorld \
           --repo-type dataset \
           --local-dir /path/to/DATA_PATH
```
For downloading specific files (instead of the full dataset), please refer to the [`dowanload_specific.py`](scripts/dowanload_specific.py).



## ğŸš€ Visualize as Point Cloud

This script allows you to convert a scene from the dataset into a 3D point cloud for inspection.

### 1\. Prerequisites

Please follow the instructions in the "Dataset Download" section to acquire the dataset.

### 2\. Data Structure

Ensure your data is structured correctly. Each scene directory should contain the following subdirectories and files:

```
<your-data-path>/b04f88d1f85a/
â”œâ”€ color/              # RGB frames (.png)
â”œâ”€ depth/              # 16-bit depth maps
â”œâ”€ flow/               # flow_u_16.png / flow_v_16.png / flow_vis.png
â”œâ”€ camera/             # split_*.json (intrinsics + extrinsics)
â”œâ”€ subject_masks/      # foreground masks (per split)
â”œâ”€ gdino_mask/         # dynamic-object masks (per frame)
â”œâ”€ text/               # structured captions (81-frame segments)
â”œâ”€ droidclib/          # coarse camera poses (if you need them)
â”œâ”€ fps.txt             # source video framerate
â””â”€ split_info.json     # how frames are grouped into splits
```

### 3\. Usage

Run the `visualize_pcd.py` script, providing the path to the scene and the desired split index.

**Example:**

```bash
python scripts/visualize_pcd.py <your-data-path>/b04f88d1f85a --split_idx 0
```

The output point cloud will be saved to `<your-data-path>/b04f88d1f85a/split0_points.ply`. You can view this file using a 3D viewer like [MeshLab](https://www.meshlab.net/).

## ğŸ“„ License
The OmniWorld dataset is released under the **Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0)**. By accessing or using this dataset, you agree to be bound by the terms and conditions outlined in this license, as well as the specific provisions detailed below.

- **Special Note on Third-Party Content**:
A portion of this dataset is derived from third-party game content. All intellectual property rights pertaining to these original game assets (including, but not limited to, RGB and depth images) remain with their respective original game developers and publishers.

- **Permitted Uses**:
You are hereby granted permission, free of charge, to use, reproduce, and share the OmniWorld dataset and any adaptations thereof, solely for non-commercial research and educational purposes. This includes, but is not limited to: academic publications, algorithm benchmarking, reproduction of scientific results.

Under this license, you are expressly **forbidden** from:

- Using the dataset, in whole or in part, for any commercial purpose, including but not limited to its incorporation into commercial products, services, or monetized applications.

- Redistributing the original third-party game assets contained within the dataset outside the scope of legitimate research sharing.
Removing or altering any copyright, license, or attribution notices.

The authors of the OmniWorld dataset provide this dataset "as is" and make no representations or warranties regarding the legality of the underlying data for any specific purpose. Users are solely responsible for ensuring that their use of the dataset complies with all applicable laws and the terms of service or license agreements of the original game publishers (sources of third-party content).

For the full legal text of the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, please visit: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode.

## Citation
If you found this dataset useful, please cite our paper
```bibtex
@misc{zhou2025omniworld,
      title={OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling}, 
      author={Yang Zhou and Yifan Wang and Jianjun Zhou and Wenzheng Chang and Haoyu Guo and Zizun Li and Kaijing Ma and Xinyue Li and Yating Wang and Haoyi Zhu and Mingyu Liu and Dingning Liu and Jiange Yang and Zhoujie Fu and Junyi Chen and Chunhua Shen and Jiangmiao Pang and Kaipeng Zhang and Tong He},
      year={2025},
      eprint={2509.12201},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2509.12201}, 
}
```